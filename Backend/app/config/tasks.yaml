# Backend/app/config/tasks.yaml
# Refactored to a flat structure where keys match method names in crew.py
# Using '>' for multi-line strings as per documentation.
# Assuming 'agent' field takes a single agent name string.
---
process_new_user_query_and_resolve_context:
  description: >
    The initial entry point for a new user query.
    1. Understand the user's immediate query.
    2. Use SessionRetrievalTool to get image interaction context (aliases, sequence, focus).
    3. Use its LLM to resolve image references in the query (e.g., "this image" -> image_hash_123).
    4. Build/update the conversation context string using ContextChainBuilder.
    5. If the query is simple and directly answerable with session context, answer it.
    6. Otherwise, identify the primary analytical task(s) needed and prepare for delegation.
  agent: session_context_manager # Agent's method name
  expected_output: >
    A JSON string containing:
    - "resolved_query": The user's query with image references replaced by unique IDs.
    - "identified_image_ids": A list of unique image IDs relevant to the query.
    - "conversation_context_summary": A brief summary of the current conversation state.
    - "next_action": A string indicating 'answer_directly', 'delegate_to_task', or 'decompose_query'.
    - "direct_answer": (Optional) The answer if 'next_action' is 'answer_directly'.
    - "delegation_task_name": (Optional) The name of the single task to delegate to.
    - "delegation_input": (Optional) A dictionary of inputs for the delegation task.

extract_base_image_metadata:
  description: >
    Extracts all available raw EXIF, IPTC, and XMP metadata for specified image(s). 
    This is a foundational step.
  agent: technical_analyzer 
  expected_output: >
    A JSON string representing a dictionary where keys are image IDs (e.g., hashes) and values are 
    comprehensive raw metadata objects (dictionaries) extracted for each image.
    Example: {"image_hash_1": {"EXIF": {...}, "IPTC": {...}}, "image_hash_2": {...}}

validate_and_normalize_metadata:
  description: >
    Validates extracted raw metadata, normalizes key fields (dates, units, etc.), 
    and generates content hashes for specified image(s).
  agent: metadata_digestor 
  expected_output: >
    A JSON string representing a dictionary where keys are image IDs. Each value is a dictionary 
    containing the 'image_hash' (content hash) and a 'processed_metadata' object. 
    The 'processed_metadata' includes validated and normalized fields, alongside the original (or a subset of) metadata.
    Example: {"image_id_1": {"image_hash": "sha256_...", "processed_metadata": {"iso": 100, "aperture_value": 2.8, ...}}}

analyze_image_temporal_properties:
  description: >
    Analyzes temporal aspects of specified image(s), including capture date/time, 
    time of day, day/night, duration since capture, and solar position (e.g., golden hour).
  agent: temporal_specialist
  expected_output: >
    A JSON string containing a list of objects, one for each image analyzed. Each object includes:
    - "image_id": The unique identifier of the image.
    - "capture_datetime_utc": Formatted capture datetime in UTC.
    - "time_of_day_category": e.g., "Morning", "Afternoon", "Evening", "Night".
    - "age_of_image": Human-readable string (e.g., "3 months ago").
    - "solar_analysis": (Optional) Object with "is_golden_hour", "is_blue_hour", "sun_azimuth", "sun_elevation".
    - "other_temporal_notes": (Optional) Any other relevant temporal observations.

analyze_image_geospatial_properties:
  description: >
    Analyzes geospatial aspects of specified image(s) with GPS data. Includes reverse 
    geocoding to address, landmark identification, and GPS coordinate presentation.
  agent: geospatial_engine
  expected_output: >
    A JSON string containing a list of objects, one for each image analyzed. Each object includes:
    - "image_id": The unique identifier of the image.
    - "gps_coordinates": {"latitude": float, "longitude": float}.
    - "address": (Optional) Human-readable address from reverse geocoding.
    - "identified_landmarks": (Optional) List of nearby landmarks, each with "name" and "distance".
    - "geospatial_summary": A brief textual summary of the location.

analyze_image_technical_details:
  description: >
    Provides detailed technical photography parameters for specified image(s), including 
    camera/lens info, exposure settings (ISO, aperture, shutter speed), resolution, 
    flash usage, and software tags.
  agent: technical_analyzer
  expected_output: >
    A JSON string containing a list of objects, one for each image analyzed. Each object includes:
    - "image_id": The unique identifier of the image.
    - "camera_make_model": String (e.g., "Sony ILCE-7M3").
    - "lens_info": String (e.g., "FE 24-70mm F2.8 GM").
    - "exposure_settings": {"iso": int, "aperture": float, "shutter_speed": str, "focal_length": str, "exposure_compensation": str}.
    - "resolution": {"width": int, "height": int}.
    - "flash_used": Boolean.
    - "software": (Optional) String identifying editing software.
    - "technical_summary": A brief textual summary of key settings.

get_environmental_context:
  description: >
    Fetches historical weather conditions (temperature, precipitation, wind, cloud cover, etc.) 
    for specified image(s) based on their capture date and location.
  agent: environmental_analyst
  expected_output: >
    A JSON string containing a list of objects, one for each image queried. Each object includes:
    - "image_id": The unique identifier of the image.
    - "queried_date": The date for which weather was fetched.
    - "queried_location": {"latitude": float, "longitude": float}.
    - "weather_data": A dictionary of weather parameters (e.g., "temp", "conditions", "windspeed", "precipitation", "sunrise", "sunset").
    - "error": (Optional) Error message if data could not be fetched for this image.

compare_images_technical_metadata:
  description: >
    Compares two or more specified images based on their technical metadata parameters 
    (e.g., ISO, aperture, shutter speed, focal length). Provides a comparison matrix and deviation scores.
  agent: comparative_engine
  expected_output: >
    A JSON string (output from MatrixComparatorTool) containing:
    - "success": Boolean.
    - "comparison_matrix": A list of dictionaries, each representing an image and its values for the compared fields (also includes normalized values and deviations if scored).
    - "image_scores": (Optional) A list of objects, each with "image_id" and "score", if scoring was performed.
    - "summary": A textual summary of the comparison.
    - "logs": (Optional) Logging information from the tool.

compare_images_temporal_aspects:
  description: >
    Compares temporal aspects of two or more images, such as identifying which was 
    taken first or the time difference between them.
  agent: temporal_specialist
  expected_output: >
    A JSON string containing:
    - "comparison_type": e.g., "order_assessment", "time_difference".
    - "image_order": (Optional) List of image_ids sorted by capture time.
    - "time_difference_seconds": (Optional) Integer, time difference in seconds between two specified images.
    - "same_day_check": (Optional) {"images_compared": [id1, id2], "are_same_day": boolean}.
    - "summary": A textual summary of the temporal comparison.
  
compare_images_geospatial_aspects:
  description: >
    Compares geospatial aspects of two or more images, such as whether they were 
    taken at the same location or the distance between their capture locations.
  agent: geospatial_engine
  expected_output: >
    A JSON string containing:
    - "images_compared": List of image_ids.
    - "distance_between_meters": (Optional) Float, if two locations are compared.
    - "same_location_assessment": (Optional) {"is_same_general_location": boolean, "criteria": "e.g., within X meters or same geocoded address"}.
    - "location_details": List of objects, each with "image_id" and its "address" or "gps_coordinates".
    - "summary": A textual summary of the geospatial comparison.

detect_image_sequences:
  description: >
    Analyzes a set of images to detect if they form a temporal sequence 
    (e.g., timelapse, burst) based on their timestamps.
  agent: temporal_specialist
  expected_output: >
    A JSON string containing:
    - "image_ids_analyzed": List of image_ids.
    - "detected_sequences": A list of sequences, where each sequence is a list of image_ids.
    - "sequence_count": Integer, number of sequences found.
    - "summary": A textual summary of findings (e.g., "A burst sequence of 5 images was detected starting from image_X.").

decompose_complex_query:
  description: >
    Analyzes a complex user query, breaks it down into multiple sub-tasks for 
    different specialist agents, and determines the order of execution and data dependencies.
  agent: query_decomposer
  expected_output: >
    A JSON string representing a structured plan. The plan should include:
    - "original_query": The user's query.
    - "overall_goal": A summary of what the user wants to achieve.
    - "sub_tasks": A list of objects, each representing a sub-task. Each sub-task object should include:
      - "task_id": A unique identifier for this sub-task step.
      - "task_name": The name of the task to be executed (from this tasks.yaml file).
      - "agent_assigned": The agent responsible for this task_name.
      - "input_parameters": A dictionary of specific inputs needed for this task_name (e.g., list of image_ids, specific metadata fields).
      - "dependencies": (Optional) A list of task_ids that must be completed before this sub-task can start.
      - "purpose": A brief description of why this sub-task is needed.
    - "execution_summary": A brief note on the planned approach.

synthesize_response_from_analyses:
  description: >
    Gathers results from one or more analytical tasks, synthesizes them into a coherent, 
    user-friendly natural language response, and suggests visualizations if applicable.
  agent: response_synthesizer
  expected_output: >
    A JSON string containing:
    - "final_answer_text": The complete, synthesized natural language response for the user.
    - "visualization_suggestions": (Optional) A list of suggested visualization types (e.g., ["table", "map"]) based on VisualizationCreatorTool's output.
    - "sources_consulted": (Optional) A list of task names or data sources used to formulate the answer.
    - "confidence_level": (Optional) A qualitative assessment (e.g., "High", "Medium") if applicable.

handle_unresolved_query_or_error:
  description: >
    Manages situations where a query cannot be understood, a task fails, or necessary 
    data is missing. Provides a helpful explanation and suggestions to the user.
  agent: fallback_handler
  expected_output: >
    A JSON string containing:
    - "user_facing_message": A clear, empathetic message explaining the issue or limitation.
    - "error_summary": (Optional) A brief internal summary of the error if applicable.
    - "suggested_actions": (Optional) A list of actionable suggestions for the user (e.g., "Try rephrasing your question.", "Ensure the image has GPS data for location queries.").
    - "log_reference_id": (Optional) An ID that can be used to trace detailed logs if needed.